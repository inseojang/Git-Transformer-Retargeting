{
    "n_enc_vocab": 0,
    "n_dec_vocab": 0,
    "n_enc_seq": 256,
    "n_dec_seq": 256,
    "n_layer": 6,
    "d_hidn": 256,
    "i_pad": 0,
    "n_head": 4,
    "d_head": 64,
    "dropout": 0.1,
    "layer_norm_epsilon": 1e-12,
    "batch_size": 64,
    "learning_rate": 5e-5,
    "n_epoch": 10,
    "n_output": 0
}
